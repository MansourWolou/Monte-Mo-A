{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "!pip install -q transformers\n",
        "!pip install -q torchaudio\n",
        "!pip install -q moviepy\n",
        "!pip install -q pillow\n",
        "!pip install -q requests\n",
        "!pip install -q librosa\n",
        "!pip install -q soundfile\n",
        "!pip install -q accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKXFM6FVku3z",
        "outputId": "0e066ffa-b19b-4567-ac52-e42f3c138bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "ak8-D70kkive",
        "outputId": "cda6f6c2-fc2b-419b-bff3-61b1facf439f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gradio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-abf4e067a1e3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import gradio as gr\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline, WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "class DescripteurVideoAutomatique:\n",
        "    def __init__(self, intervalle=5, top_n=10, utiliser_cuda=True):\n",
        "        self.intervalle = intervalle\n",
        "        self.top_n = top_n\n",
        "        self.utiliser_cuda = utiliser_cuda\n",
        "        self.device = \"cuda\" if utiliser_cuda and torch.cuda.is_available() else \"cpu\"\n",
        "        self.chemin_video = None\n",
        "        self.scenes = []\n",
        "        self.descriptions = []\n",
        "        self.api_key_mistral = \"JpPD7Mcbkn4kt9EASK8FyXKT0s8zdNn5\"\n",
        "        self.bibliotheque_path = \"bibliotheque_videos.json\"\n",
        "        self.bibliotheque = self.charger_bibliotheque()\n",
        "\n",
        "    def charger_bibliotheque(self):\n",
        "        if os.path.exists(self.bibliotheque_path):\n",
        "            try:\n",
        "                with open(self.bibliotheque_path, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except:\n",
        "                return []\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def sauvegarder_bibliotheque(self):\n",
        "        with open(self.bibliotheque_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.bibliotheque, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    def generer_nom_descriptif(self, legendes, transcriptions):\n",
        "        \"\"\"Génère un nom descriptif à partir des légendes et transcriptions\"\"\"\n",
        "        # Combinaison des 3 premières légendes\n",
        "        nom_base = \" - \".join([leg.split(\".\")[0] for leg in legendes[:3] if leg])\n",
        "        # Limiter la longueur du nom\n",
        "        if not nom_base:\n",
        "            nom_base = \"Vidéo sans description\"\n",
        "        return nom_base[:50] + \"...\" if len(nom_base) > 50 else nom_base\n",
        "\n",
        "    def ajouter_a_bibliotheque(self, video_path, resultats, nom_personnalise=None):\n",
        "        nom_fichier = os.path.basename(video_path)\n",
        "        date_analyse = datetime.now().strftime(\"%d/%m/%Y %H:%M\")\n",
        "\n",
        "        # Extraction du premier moment intéressant\n",
        "        moment_interessant = \"Aucun moment intéressant trouvé\"\n",
        "        if resultats:\n",
        "            top = max(resultats, key=lambda x: x[\"score\"])\n",
        "            moment_interessant = f\"{self.formater_temps(top['debut'])}-{self.formater_temps(top['fin'])}\"\n",
        "\n",
        "        # Génération d'un nom descriptif si aucun nom personnalisé n'est fourni\n",
        "        if not nom_personnalise:\n",
        "            legendes = [r[\"legende\"] for r in resultats[:3]] if resultats else []\n",
        "            transcriptions = [r[\"transcription\"] for r in resultats[:3]] if resultats else []\n",
        "            nom_descriptif = self.generer_nom_descriptif(legendes, transcriptions)\n",
        "        else:\n",
        "            nom_descriptif = nom_personnalise\n",
        "\n",
        "        # Création d'un identifiant unique basé sur le nom et la date\n",
        "        video_id = f\"{nom_fichier}_{int(time.time())}\"\n",
        "\n",
        "        # Ajout à la bibliothèque\n",
        "        self.bibliotheque.append({\n",
        "            \"id\": video_id,\n",
        "            \"nom\": nom_fichier,\n",
        "            \"nom_descriptif\": nom_descriptif,\n",
        "            \"chemin\": video_path,\n",
        "            \"date_analyse\": date_analyse,\n",
        "            \"moment_interessant\": moment_interessant,\n",
        "            \"nombre_scenes\": len(resultats) if resultats else 0\n",
        "        })\n",
        "\n",
        "        # Sauvegarde\n",
        "        self.sauvegarder_bibliotheque()\n",
        "\n",
        "        return self.bibliotheque\n",
        "\n",
        "    def charger_modele(self):\n",
        "        print(\"📦 Chargement des modèles...\")\n",
        "        # Modèle BLIP pour la description d'images\n",
        "        self.processeur = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "        self.modele = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(self.device)\n",
        "\n",
        "\n",
        "        # Modèle Whisper pour la transcription audio\n",
        "        self.whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "        self.whisper_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(self.device)\n",
        "\n",
        "        # Traducteur\n",
        "        self.traducteur = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "        print(\"✅ Modèles chargés avec succès!\")\n",
        "\n",
        "    def extraire_scenes(self):\n",
        "        clip = VideoFileClip(self.chemin_video)\n",
        "        duree = clip.duration\n",
        "        self.scenes = [(debut, min(debut + self.intervalle, duree)) for debut in range(0, int(duree), self.intervalle)]\n",
        "\n",
        "    def obtenir_image_milieu(self, debut, fin):\n",
        "        clip = VideoFileClip(self.chemin_video).subclip(debut, fin)\n",
        "        image = clip.get_frame((fin - debut) / 2)\n",
        "        return Image.fromarray(image)\n",
        "\n",
        "    def generer_legende(self, image):\n",
        "        inputs = self.processeur(image, return_tensors=\"pt\").to(self.device)\n",
        "        sortie = self.modele.generate(**inputs)\n",
        "        legende_en = self.processeur.decode(sortie[0], skip_special_tokens=True)\n",
        "        legende_fr = self.traducteur(legende_en)[0]['translation_text']\n",
        "        return legende_fr\n",
        "\n",
        "    def extraire_audio(self, debut, fin, nom=\"scene_audio.wav\"):\n",
        "        try:\n",
        "            clip = VideoFileClip(self.chemin_video).subclip(debut, fin)\n",
        "            if clip.audio is not None:\n",
        "                audio_path = os.path.join(os.getcwd(), nom)\n",
        "                clip.audio.write_audiofile(audio_path, logger=None)\n",
        "                return audio_path\n",
        "            else:\n",
        "                print(f\"Avertissement: Pas d'audio dans la section {debut}-{fin}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur d'extraction audio: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def transcrire_audio_whisper(self, audio_path):\n",
        "        if audio_path is None:\n",
        "            return \"Pas d'audio disponible pour cette section.\"\n",
        "\n",
        "        try:\n",
        "            import librosa\n",
        "            import soundfile as sf\n",
        "\n",
        "            # Chargement de l'audio\n",
        "            audio, rate = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "            # Convertir en format attendu par Whisper\n",
        "            input_features = self.whisper_processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features.to(self.device)\n",
        "\n",
        "            # Générer les tokens de prédiction\n",
        "            predicted_ids = self.whisper_model.generate(input_features, language=\"fr\", task=\"transcribe\")\n",
        "\n",
        "            # Décodage de la transcription\n",
        "            transcription = self.whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "            if not transcription:\n",
        "                return \"Aucune transcription détectée.\"\n",
        "\n",
        "            return transcription\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Erreur de transcription Whisper: {str(e)}\"\n",
        "        finally:\n",
        "            # Nettoyage du fichier\n",
        "            try:\n",
        "                if os.path.exists(audio_path):\n",
        "                    os.remove(audio_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    def demander_a_mistral(self, prompt, contexte=\"\"):\n",
        "        url = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.api_key_mistral}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Vous êtes un assistant spécialisé dans l'analyse vidéo. Répondez en français.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Contexte sur la vidéo: {contexte}\\n\\nQuestion: {prompt}\"}\n",
        "        ]\n",
        "\n",
        "        data = {\n",
        "            \"model\": \"mistral-large-latest\",\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": 0.7,\n",
        "            \"max_tokens\": 800\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data)\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except Exception as e:\n",
        "            return f\"Erreur lors de la communication avec Mistral: {str(e)}\"\n",
        "\n",
        "    def decrire_video(self):\n",
        "        self.extraire_scenes()\n",
        "        self.descriptions = []\n",
        "\n",
        "        for debut, fin in self.scenes:\n",
        "            print(f\"Analyse de la scène {self.formater_temps(debut)}-{self.formater_temps(fin)}...\")\n",
        "\n",
        "            # Génération de la légende visuelle\n",
        "            image = self.obtenir_image_milieu(debut, fin)\n",
        "            legende = self.generer_legende(image)\n",
        "\n",
        "            # Extraction et transcription audio avec Whisper\n",
        "            audio_path = self.extraire_audio(debut, fin, nom=f\"audio_{int(debut)}.wav\")\n",
        "            transcription = self.transcrire_audio_whisper(audio_path)\n",
        "\n",
        "            # Calcul du score (pondération ajustée pour favoriser les scènes avec audio)\n",
        "            score = len(legende) * 0.05 + len(transcription) * 0.15 + np.random.rand() * 0.3\n",
        "\n",
        "            self.descriptions.append({\n",
        "                \"debut\": debut,\n",
        "                \"fin\": fin,\n",
        "                \"legende\": legende,\n",
        "                \"transcription\": transcription,\n",
        "                \"score\": score\n",
        "            })\n",
        "\n",
        "    def obtenir_meilleures_scenes(self):\n",
        "        return sorted(self.descriptions, key=lambda x: x[\"score\"], reverse=True)[:self.top_n]\n",
        "\n",
        "    def formater_temps(self, secondes):\n",
        "        return f\"{int(secondes // 60):02d}:{int(secondes % 60):02d}\"\n",
        "\n",
        "    def trouver_moment_interessant(self, scenes):\n",
        "        if not scenes:\n",
        "            return \"Aucun moment intéressant trouvé\"\n",
        "\n",
        "        top = max(scenes, key=lambda x: x[\"score\"])\n",
        "        return f\"Le moment le plus intéressant se trouve entre {self.formater_temps(top['debut'])} et {self.formater_temps(top['fin'])}\"\n",
        "\n",
        "    def analyser_video(self, video_path, nom_personnalise=None):\n",
        "        if not video_path:\n",
        "            return None, \"Veuillez télécharger une vidéo.\", None\n",
        "\n",
        "        # Sauvegarde du chemin de la vidéo\n",
        "        self.chemin_video = video_path\n",
        "\n",
        "        # Chargement du modèle si nécessaire\n",
        "        if not hasattr(self, 'modele') or not hasattr(self, 'whisper_model'):\n",
        "            self.charger_modele()\n",
        "\n",
        "        # Analyse de la vidéo\n",
        "        try:\n",
        "            self.decrire_video()\n",
        "            meilleures_scenes = self.obtenir_meilleures_scenes()\n",
        "            moment_interessant = self.trouver_moment_interessant(meilleures_scenes)\n",
        "\n",
        "            # Création d'un DataFrame pour l'affichage\n",
        "            df = pd.DataFrame([{\n",
        "                \"Rang\": i+1,\n",
        "                \"Début\": self.formater_temps(s[\"debut\"]),\n",
        "                \"Fin\": self.formater_temps(s[\"fin\"]),\n",
        "                \"Description\": s[\"legende\"],\n",
        "                \"Transcription audio\": s[\"transcription\"]\n",
        "            } for i, s in enumerate(meilleures_scenes)])\n",
        "\n",
        "            # Obtenir le nom de fichier de la vidéo pour l'affichage\n",
        "            nom_fichier = os.path.basename(video_path)\n",
        "\n",
        "            # Ajout à la bibliothèque\n",
        "            bibliotheque_mise_a_jour = self.ajouter_a_bibliotheque(video_path, self.descriptions, nom_personnalise)\n",
        "\n",
        "            return df, f\"Vidéo: {nom_fichier}\\n{moment_interessant}\", bibliotheque_mise_a_jour\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"Erreur lors de l'analyse: {str(e)}\", None\n",
        "\n",
        "    def analyser_et_demander(self, video_path, prompt, nom_personnalise=None):\n",
        "        df, message, bibliotheque = self.analyser_video(video_path, nom_personnalise)\n",
        "\n",
        "        if df is None:\n",
        "            return None, message, None, bibliotheque\n",
        "\n",
        "        # Préparation du contexte pour Mistral\n",
        "        contexte = \"Informations sur les scènes principales: \"\n",
        "        for i, row in df.head(3).iterrows():\n",
        "            contexte += f\"\\nScène {row['Début']}-{row['Fin']}: {row['Description']}. Audio: {row['Transcription audio']}\"\n",
        "\n",
        "        # Interrogation de Mistral\n",
        "        reponse_mistral = self.demander_a_mistral(prompt, contexte)\n",
        "\n",
        "        return df, message, reponse_mistral, bibliotheque\n",
        "\n",
        "# Interface Gradio\n",
        "def creer_interface():\n",
        "    # Création d'un descripteur partagé\n",
        "    descr = DescripteurVideoAutomatique()\n",
        "\n",
        "    # Fonctions pour la manipulation de la bibliothèque\n",
        "    def charger_bibliotheque():\n",
        "        return pd.DataFrame([\n",
        "            {\"Index\": i, \"ID\": item[\"id\"], \"Nom\": item[\"nom\"],\n",
        "             \"Nom descriptif\": item.get(\"nom_descriptif\", \"\"),\n",
        "             \"Date d'analyse\": item[\"date_analyse\"],\n",
        "             \"Moment intéressant\": item[\"moment_interessant\"],\n",
        "             \"Nombre de scènes\": item[\"nombre_scenes\"]}\n",
        "            for i, item in enumerate(descr.charger_bibliotheque())\n",
        "        ])\n",
        "\n",
        "    def supprimer_de_bibliotheque(index):\n",
        "        try:\n",
        "            index = int(index)\n",
        "            if 0 <= index < len(descr.bibliotheque):\n",
        "                descr.bibliotheque.pop(index)\n",
        "                descr.sauvegarder_bibliotheque()\n",
        "            return charger_bibliotheque()\n",
        "        except:\n",
        "            return charger_bibliotheque()\n",
        "\n",
        "    def ajouter_video_a_bibliotheque(video, nom_descriptif):\n",
        "        if not video:\n",
        "            return \"Veuillez télécharger une vidéo.\", charger_bibliotheque()\n",
        "\n",
        "        try:\n",
        "            # Analyse simplifiée (juste pour l'extraction des scènes)\n",
        "            descr.chemin_video = video\n",
        "            # S'assurer que les modèles sont chargés\n",
        "            if not hasattr(descr, 'modele') or not hasattr(descr, 'whisper_model'):\n",
        "                descr.charger_modele()\n",
        "\n",
        "            descr.extraire_scenes()\n",
        "            # Analyse de quelques scènes clés seulement pour un traitement plus rapide\n",
        "            scenes_echantillon = descr.scenes[:3]\n",
        "            descriptions = []\n",
        "\n",
        "            for debut, fin in scenes_echantillon:\n",
        "                image = descr.obtenir_image_milieu(debut, fin)\n",
        "                legende = descr.generer_legende(image)\n",
        "                descriptions.append({\n",
        "                    \"debut\": debut,\n",
        "                    \"fin\": fin,\n",
        "                    \"legende\": legende,\n",
        "                    \"transcription\": \"\",\n",
        "                    \"score\": len(legende) * 0.05 + np.random.rand() * 0.3\n",
        "                })\n",
        "\n",
        "            # Ajout à la bibliothèque avec le nom personnalisé\n",
        "            descr.ajouter_a_bibliotheque(video, descriptions, nom_descriptif or None)\n",
        "            return f\"Vidéo ajoutée avec succès: {os.path.basename(video)}\", charger_bibliotheque()\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Erreur lors de l'ajout: {str(e)}\", charger_bibliotheque()\n",
        "\n",
        "    # Fonction d'analyse pour une vidéo avec prompt\n",
        "    def analyser_avec_prompt(video, prompt, nom_personnalise):\n",
        "        if not video:\n",
        "            return None, \"Veuillez télécharger une vidéo.\", \"\", None\n",
        "        return descr.analyser_et_demander(video, prompt, nom_personnalise)\n",
        "\n",
        "    # Style CSS personnalisé pour la barre de progrès type Canva\n",
        "    css_personnalise = \"\"\"\n",
        "    .video-player {\n",
        "        max-width: 640px;\n",
        "        max-height: 360px;\n",
        "        margin: 0 auto;\n",
        "    }\n",
        "\n",
        "    .progress-bar-container {\n",
        "        width: 100%;\n",
        "        height: 10px;\n",
        "        background-color: #ddd;\n",
        "        border-radius: 5px;\n",
        "        overflow: hidden;\n",
        "        margin-top: -15px;\n",
        "        position: relative;\n",
        "        z-index: 10;\n",
        "    }\n",
        "\n",
        "    .progress-bar {\n",
        "        height: 100%;\n",
        "        background-color: #ff5722;\n",
        "        width: 0%;\n",
        "        transition: width 0.3s;\n",
        "    }\n",
        "\n",
        "    .time-display {\n",
        "        font-size: 14px;\n",
        "        margin-top: 5px;\n",
        "        text-align: center;\n",
        "    }\n",
        "\n",
        "    .video-card {\n",
        "        border: 1px solid #ddd;\n",
        "        border-radius: 10px;\n",
        "        padding: 10px;\n",
        "        margin-bottom: 15px;\n",
        "        background-color: #f9f9f9;\n",
        "    }\n",
        "\n",
        "    .bibliotheque-item {\n",
        "        padding: 10px;\n",
        "        border-bottom: 1px solid #eee;\n",
        "        border-radius: 5px;\n",
        "        margin-bottom: 5px;\n",
        "    }\n",
        "\n",
        "    .bibliotheque-item:hover {\n",
        "        background-color: #f0f0f0;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    # Création de l'interface principale\n",
        "    with gr.Blocks(css=css_personnalise, title=\"Descripteur Automatique de Vidéo\") as interface:\n",
        "        gr.Markdown(\"# Descripteur Automatique de Vidéo avec Whisper & Mistral AI\")\n",
        "\n",
        "        with gr.Tabs():\n",
        "            # Onglet Analyse de Vidéo\n",
        "            with gr.TabItem(\"Analyse de Vidéo\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=2):\n",
        "                        video_input = gr.Video(label=\"Téléchargez votre vidéo\", elem_classes=[\"video-player\"])\n",
        "                        nom_descriptif_analyse = gr.Textbox(label=\"Nom descriptif de la vidéo (optionnel)\", placeholder=\"Laissez vide pour génération automatique\")\n",
        "                        gr.HTML(\"\"\"\n",
        "                        <div class=\"progress-bar-container\">\n",
        "                            <div class=\"progress-bar\" id=\"video-progress\"></div>\n",
        "                        </div>\n",
        "                        <div class=\"time-display\" id=\"time-display\">00:00 / 00:00</div>\n",
        "                        <script>\n",
        "                            document.addEventListener('DOMContentLoaded', function() {\n",
        "                                setTimeout(function() {\n",
        "                                    const videoElements = document.querySelectorAll('video');\n",
        "                                    videoElements.forEach(function(video) {\n",
        "                                        const progressBar = document.getElementById('video-progress');\n",
        "                                        const timeDisplay = document.getElementById('time-display');\n",
        "\n",
        "                                        video.addEventListener('timeupdate', function() {\n",
        "                                            const progress = (video.currentTime / video.duration) * 100;\n",
        "                                            progressBar.style.width = progress + '%';\n",
        "\n",
        "                                            const currentMinutes = Math.floor(video.currentTime / 60);\n",
        "                                            const currentSeconds = Math.floor(video.currentTime % 60);\n",
        "                                            const totalMinutes = Math.floor(video.duration / 60);\n",
        "                                            const totalSeconds = Math.floor(video.duration % 60);\n",
        "\n",
        "                                            timeDisplay.textContent =\n",
        "                                                `${currentMinutes.toString().padStart(2, '0')}:${currentSeconds.toString().padStart(2, '0')} /\n",
        "                                                ${totalMinutes.toString().padStart(2, '0')}:${totalSeconds.toString().padStart(2, '0')}`;\n",
        "                                        });\n",
        "                                    });\n",
        "                                }, 1000);\n",
        "                            });\n",
        "                        </script>\n",
        "                        \"\"\")\n",
        "\n",
        "                    with gr.Column(scale=2):\n",
        "                        prompt_input = gr.Textbox(label=\"Posez une question sur cette vidéo\", placeholder=\"Ex: Quels sont les points clés de cette vidéo?\", lines=3)\n",
        "                        analyser_btn = gr.Button(\"Analyser la vidéo\", variant=\"primary\")\n",
        "                        gr.Markdown(\"### Réponse de Mistral AI\")\n",
        "                        reponse_mistral = gr.Textbox(label=\"\", placeholder=\"La réponse apparaîtra ici après l'analyse...\", lines=6)\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        gr.Markdown(\"### Moments clés\")\n",
        "                        resultats_df = gr.DataFrame(label=\"\", headers=[\"Rang\", \"Début\", \"Fin\", \"Description\", \"Transcription audio\"])\n",
        "                        moment_interessant = gr.Textbox(label=\"Moment le plus intéressant\")\n",
        "\n",
        "                # Traitement du clic sur le bouton d'analyse\n",
        "                analyser_btn.click(\n",
        "                    fn=analyser_avec_prompt,\n",
        "                    inputs=[video_input, prompt_input, nom_descriptif_analyse],\n",
        "                    outputs=[resultats_df, moment_interessant, reponse_mistral, gr.State()]\n",
        "                )\n",
        "\n",
        "            # Onglet Bibliothèque de Vidéos\n",
        "            with gr.TabItem(\"Bibliothèque de Vidéos\"):\n",
        "                gr.Markdown(\"## Vos vidéos analysées\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    bibliotheque_btn = gr.Button(\"Actualiser la bibliothèque\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    bibliotheque_list = gr.DataFrame(\n",
        "                        headers=[\"Index\", \"ID\", \"Nom\", \"Nom descriptif\", \"Date d'analyse\", \"Moment intéressant\", \"Nombre de scènes\"],\n",
        "                        label=\"Vidéos sauvegardées\"\n",
        "                    )\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=1):\n",
        "                        index_suppression = gr.Number(label=\"Index de la vidéo à supprimer\", precision=0)\n",
        "                        supprimer_btn = gr.Button(\"Supprimer de la bibliothèque\")\n",
        "\n",
        "                    with gr.Column(scale=2):\n",
        "                        gr.Markdown(\"### Ajouter une vidéo à la bibliothèque\")\n",
        "                        video_ajout = gr.Video(label=\"Vidéo à ajouter\")\n",
        "                        nom_descriptif = gr.Textbox(label=\"Nom descriptif\", placeholder=\"Entrez un nom descriptif pour cette vidéo\")\n",
        "                        ajouter_btn = gr.Button(\"Ajouter à la bibliothèque\", variant=\"primary\")\n",
        "                        resultat_ajout = gr.Textbox(label=\"Résultat de l'ajout\")\n",
        "\n",
        "                # Chargement initial de la bibliothèque\n",
        "                interface.load(\n",
        "                    fn=charger_bibliotheque,\n",
        "                    inputs=None,\n",
        "                    outputs=bibliotheque_list\n",
        "                )\n",
        "\n",
        "                # Actualisation de la bibliothèque\n",
        "                bibliotheque_btn.click(\n",
        "                    fn=charger_bibliotheque,\n",
        "                    inputs=None,\n",
        "                    outputs=bibliotheque_list\n",
        "                )\n",
        "\n",
        "                # Suppression d'une vidéo de la bibliothèque\n",
        "                supprimer_btn.click(\n",
        "                    fn=supprimer_de_bibliotheque,\n",
        "                    inputs=index_suppression,\n",
        "                    outputs=bibliotheque_list\n",
        "                )\n",
        "\n",
        "                # Ajout d'une vidéo à la bibliothèque\n",
        "                ajouter_btn.click(\n",
        "                    fn=ajouter_video_a_bibliotheque,\n",
        "                    inputs=[video_ajout, nom_descriptif],\n",
        "                    outputs=[resultat_ajout, bibliotheque_list]\n",
        "                )\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Pour une utilisation dans Google Colab\n",
        "if __name__ == \"__main__\":\n",
        "    # Installation des dépendances si nécessaire\n",
        "    try:\n",
        "        import gradio\n",
        "    except ImportError:\n",
        "        print(\"🔧 Installation des dépendances nécessaires...\")\n",
        "        !pip install gradio transformers moviepy pillow requests librosa soundfile -q\n",
        "\n",
        "    try:\n",
        "        from transformers import WhisperProcessor\n",
        "    except:\n",
        "        !pip install transformers[torch] -q\n",
        "\n",
        "    # Importation des bibliothèques après installation\n",
        "    import gradio as gr\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline\n",
        "    from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "    from moviepy.editor import VideoFileClip\n",
        "\n",
        "    # Lancement de l'interface\n",
        "    interface = creer_interface()\n",
        "    interface.launch(debug=True, share=True)"
      ]
    }
  ]
}